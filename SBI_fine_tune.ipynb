{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP26H5-R_sdv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Morteza-24/SelfBlendedImages.git\n",
        "!pip install efficientnet-pytorch retinaface-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv SelfBlendedImages/src/* ./"
      ],
      "metadata": {
        "id": "jZ-P7ftSEUM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir preprocess\n",
        "!wget https://github.com/codeniko/shape_predictor_81_face_landmarks/raw/refs/heads/master/shape_predictor_81_face_landmarks.dat\n",
        "!mv shape_predictor_81_face_landmarks.dat preprocess"
      ],
      "metadata": {
        "id": "LyUJeMdRBrOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir -p utils/library\n",
        "git clone https://github.com/AlgoHunt/Face-Xray.git utils/library"
      ],
      "metadata": {
        "id": "velvKyOlCClY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qMvbcHe0AGh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/FFc23.tar ./"
      ],
      "metadata": {
        "id": "2GXfjxbXAHFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import sys\n",
        "import random\n",
        "from utils.sbi import SBI_Dataset\n",
        "from utils.scheduler import LinearDecayLR\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "from utils.logs import log\n",
        "from utils.funcs import load_json\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from model import Detector\n",
        "\n",
        "\n",
        "def compute_accuray(pred,true):\n",
        "    pred_idx=pred.argmax(dim=1).cpu().data.numpy()\n",
        "    tmp=pred_idx==true.cpu().numpy()\n",
        "    return sum(tmp)/len(pred_idx)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    cfg=load_json(\"src/configs/sbi/base.json\")\n",
        "\n",
        "    seed=5\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "\n",
        "    image_size=cfg['image_size']\n",
        "    batch_size=cfg['batch_size']\n",
        "    train_dataset=SBI_Dataset(phase='train',image_size=image_size)\n",
        "    val_dataset=SBI_Dataset(phase='val',image_size=image_size)\n",
        "\n",
        "    train_loader=torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size//2,\n",
        "                        shuffle=True,\n",
        "                        collate_fn=train_dataset.collate_fn,\n",
        "                        num_workers=4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=True,\n",
        "                        worker_init_fn=train_dataset.worker_init_fn\n",
        "                        )\n",
        "    val_loader=torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        collate_fn=val_dataset.collate_fn,\n",
        "                        num_workers=4,\n",
        "                        pin_memory=True,\n",
        "                        worker_init_fn=val_dataset.worker_init_fn\n",
        "                        )\n",
        "\n",
        "    model=Detector()\n",
        "\n",
        "    model=model.to('cuda')\n",
        "\n",
        "\n",
        "\n",
        "    iter_loss=[]\n",
        "    train_losses=[]\n",
        "    test_losses=[]\n",
        "    train_accs=[]\n",
        "    test_accs=[]\n",
        "    val_accs=[]\n",
        "    val_losses=[]\n",
        "    n_epoch=cfg['epoch']\n",
        "    lr_scheduler=LinearDecayLR(model.optimizer, n_epoch, int(n_epoch/4*3))\n",
        "    last_loss=99999\n",
        "\n",
        "\n",
        "    now=datetime.now()\n",
        "    save_path='output/{}_'.format(args.session_name)+now.strftime(os.path.splitext(os.path.basename(args.config))[0])+'_'+now.strftime(\"%m_%d_%H_%M_%S\")+'/'\n",
        "    os.mkdir(save_path)\n",
        "    os.mkdir(save_path+'weights/')\n",
        "    os.mkdir(save_path+'logs/')\n",
        "    logger = log(path=save_path+\"logs/\", file=\"losses.logs\")\n",
        "\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    last_auc=0\n",
        "    last_val_auc=0\n",
        "    weight_dict={}\n",
        "    n_weight=5\n",
        "    for epoch in range(n_epoch):\n",
        "        np.random.seed(seed + epoch)\n",
        "        train_loss=0.\n",
        "        train_acc=0.\n",
        "        model.train(mode=True)\n",
        "        for step,data in enumerate(tqdm(train_loader)):\n",
        "            img=data['img'].to(device, non_blocking=True).float()\n",
        "            target=data['label'].to(device, non_blocking=True).long()\n",
        "            output=model.training_step(img, target)\n",
        "            loss=criterion(output,target)\n",
        "            loss_value=loss.item()\n",
        "            iter_loss.append(loss_value)\n",
        "            train_loss+=loss_value\n",
        "            acc=compute_accuray(F.log_softmax(output,dim=1),target)\n",
        "            train_acc+=acc\n",
        "        lr_scheduler.step()\n",
        "        train_losses.append(train_loss/len(train_loader))\n",
        "        train_accs.append(train_acc/len(train_loader))\n",
        "\n",
        "        log_text=\"Epoch {}/{} | train loss: {:.4f}, train acc: {:.4f}, \".format(\n",
        "                        epoch+1,\n",
        "                        n_epoch,\n",
        "                        train_loss/len(train_loader),\n",
        "                        train_acc/len(train_loader),\n",
        "                        )\n",
        "\n",
        "        model.train(mode=False)\n",
        "        val_loss=0.\n",
        "        val_acc=0.\n",
        "        output_dict=[]\n",
        "        target_dict=[]\n",
        "        np.random.seed(seed)\n",
        "        for step,data in enumerate(tqdm(val_loader)):\n",
        "            img=data['img'].to(device, non_blocking=True).float()\n",
        "            target=data['label'].to(device, non_blocking=True).long()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output=model(img)\n",
        "                loss=criterion(output,target)\n",
        "\n",
        "            loss_value=loss.item()\n",
        "            iter_loss.append(loss_value)\n",
        "            val_loss+=loss_value\n",
        "            acc=compute_accuray(F.log_softmax(output,dim=1),target)\n",
        "            val_acc+=acc\n",
        "            output_dict+=output.softmax(1)[:,1].cpu().data.numpy().tolist()\n",
        "            target_dict+=target.cpu().data.numpy().tolist()\n",
        "        val_losses.append(val_loss/len(val_loader))\n",
        "        val_accs.append(val_acc/len(val_loader))\n",
        "        val_auc=roc_auc_score(target_dict,output_dict)\n",
        "        log_text+=\"val loss: {:.4f}, val acc: {:.4f}, val auc: {:.4f}\".format(\n",
        "                        val_loss/len(val_loader),\n",
        "                        val_acc/len(val_loader),\n",
        "                        val_auc\n",
        "                        )\n",
        "\n",
        "\n",
        "        if len(weight_dict)<n_weight:\n",
        "            save_model_path=os.path.join(save_path+'weights/',\"{}_{:.4f}_val.tar\".format(epoch+1,val_auc))\n",
        "            weight_dict[save_model_path]=val_auc\n",
        "            torch.save({\n",
        "                    \"model\":model.state_dict(),\n",
        "                    \"optimizer\":model.optimizer.state_dict(),\n",
        "                    \"epoch\":epoch\n",
        "                },save_model_path)\n",
        "            last_val_auc=min([weight_dict[k] for k in weight_dict])\n",
        "\n",
        "        elif val_auc>=last_val_auc:\n",
        "            save_model_path=os.path.join(save_path+'weights/',\"{}_{:.4f}_val.tar\".format(epoch+1,val_auc))\n",
        "            for k in weight_dict:\n",
        "                if weight_dict[k]==last_val_auc:\n",
        "                    del weight_dict[k]\n",
        "                    os.remove(k)\n",
        "                    weight_dict[save_model_path]=val_auc\n",
        "                    break\n",
        "            torch.save({\n",
        "                    \"model\":model.state_dict(),\n",
        "                    \"optimizer\":model.optimizer.state_dict(),\n",
        "                    \"epoch\":epoch\n",
        "                },save_model_path)\n",
        "            last_val_auc=min([weight_dict[k] for k in weight_dict])\n",
        "\n",
        "        logger.info(log_text)\n"
      ],
      "metadata": {
        "id": "_dXr04J_AJjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}